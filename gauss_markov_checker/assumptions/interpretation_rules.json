{
    "variance_inflation_factor": "The Variance Inflation Factor (VIF) measures how much the variance of a regression coefficient is inflated due to multicollinearity. A VIF of 1 indicates no correlation between a predictor and other variables. Values between 1 and 5 suggest moderate correlation but are generally acceptable. VIFs between 5 and 10 indicate high correlation that may be problematic and warrant investigation. VIFs above 10 signal severe multicollinearity that requires corrective action such as removing variables, combining correlated predictors, or using regularization techniques. The tolerance (1/VIF) provides an alternative interpretation: values below 0.1 indicate severe multicollinearity, while values above 0.2 are generally acceptable.",

    "condition_number": "The condition number (κ) provides a global measure of multicollinearity in the model. It is calculated as the ratio of the largest to smallest eigenvalue of the correlation matrix. A condition number below 10 indicates weak multicollinearity and requires no action. Values between 10 and 30 suggest moderate multicollinearity, warranting caution in coefficient interpretation. Numbers between 30 and 100 indicate strong multicollinearity requiring remedial measures. Condition numbers above 100 signal severe multicollinearity demanding immediate attention. When examining variance decomposition proportions, if two or more variables have proportions exceeding 0.5 for components with high condition indices (>30), these variables are likely the source of multicollinearity.",

    "correlation_analysis": "Correlation analysis examines pairwise relationships between predictor variables to identify potential multicollinearity. Correlations (r) with absolute values below 0.3 indicate weak relationships requiring no action. Values between 0.3 and 0.6 suggest moderate correlation that should be monitored but is generally acceptable. Correlations between 0.6 and 0.8 indicate strong relationships warranting review for potential redundancy. Absolute correlations of 0.8 or higher signal very strong relationships, and removal of one of the correlated variables should be considered. However, correlation analysis alone may miss complex multicollinearity involving multiple variables.",

    "reset_test": "Ramsey's RESET (Regression Equation Specification Error Test) examines whether non-linear combinations of the fitted values help explain the response variable, indicating model misspecification. The null hypothesis is that the model is correctly specified (linear relationship). If the p-value is less than 0.05, we reject the null hypothesis, suggesting non-linear relationships in the data. This indicates that the current linear specification may be inadequate, and non-linear transformations of variables or alternative model specifications should be considered. A p-value greater than 0.05 suggests the linear specification is appropriate. The test should be complemented with visual diagnostics: residual plots should show random scatter around zero, and component-plus-residual plots should display approximately linear relationships.",

    "breusch_pagan": "The Breusch-Pagan test examines whether the variance of the residuals is constant (homoscedasticity). Under the null hypothesis of homoscedasticity, the variance of the residuals should not systematically vary with any predictor variable or fitted values. A p-value less than 0.05 rejects the null hypothesis, indicating heteroscedasticity (non-constant variance). This violation of the Gauss-Markov assumptions means standard errors may be biased, affecting inference. Solutions include using heteroscedasticity-robust standard errors, weighted least squares, or variance-stabilizing transformations. A p-value greater than 0.05 suggests no significant evidence of heteroscedasticity.",

    "white_test": "White's test is a more general test for heteroscedasticity that does not assume a specific form of heteroscedasticity. It includes squared terms and cross-products of the independent variables, making it more robust than the Breusch-Pagan test. The null hypothesis is homoscedasticity (constant variance). A p-value less than 0.05 rejects the null hypothesis, indicating heteroscedasticity. White's test is particularly useful when the form of heteroscedasticity is unknown or complex. However, in small samples, it may have lower power due to the large number of regressors in the auxiliary regression. When heteroscedasticity is detected, White's heteroscedasticity-consistent standard errors can be used for inference without changing the coefficient estimates.",

    "goldfeld_quandt": "The Goldfeld-Quandt test compares the variance of residuals in two subsets of the data to detect heteroscedasticity. The data is typically ordered by a variable suspected of being related to the error variance, then split into two groups (omitting some middle observations). The null hypothesis is homoscedasticity. A p-value less than 0.05 rejects the null hypothesis, indicating heteroscedasticity. The test statistic is the ratio of the error sum of squares from the two regressions, which follows an F-distribution. The test is particularly useful when heteroscedasticity is suspected to be related to a specific variable. The 'alternative' parameter can specify 'increasing', 'decreasing', or 'two-sided' variance, allowing for directional testing. This test is less general than White's test but can be more powerful when the form of heteroscedasticity is correctly specified.",

    "bartlett_test": "Bartlett's test examines whether multiple groups of residuals have equal variances. The null hypothesis is that all groups have equal variances. A p-value less than 0.05 rejects the null hypothesis, indicating heteroscedasticity. The test is particularly useful when residuals can be naturally grouped, such as by quantiles of fitted values or by categories of a predictor variable. Bartlett's test is sensitive to departures from normality, so it should be used with caution when the residuals are not normally distributed. When heteroscedasticity is detected, consider using robust standard errors, weighted least squares, or transforming the dependent variable. The test complements visual inspection of residual plots by providing a formal statistical assessment of variance equality.",

    "levene_test": "Levene's test checks for equal variances across groups of residuals and is more robust to non-normality than Bartlett's test. The null hypothesis is that all groups have equal variances. A p-value less than 0.05 rejects the null hypothesis, indicating heteroscedasticity. The test works by performing an ANOVA on the absolute deviations of residuals from their group means (or medians). The 'center' parameter can be set to 'mean', 'median' (default, also known as Brown-Forsythe test), or 'trimmed' to handle different types of distributions. Levene's test is preferred when residuals may not be normally distributed. When heteroscedasticity is detected, consider using robust standard errors, generalized least squares, or transforming variables. This test is particularly valuable in econometric analysis where normality assumptions are often violated.",

    "durbin_watson": "The Durbin-Watson test detects first-order autocorrelation in regression residuals. The test statistic (d) ranges from 0 to 4. Values near 2 indicate no autocorrelation. Values approaching 0 suggest strong positive autocorrelation, while values near 4 indicate strong negative autocorrelation. As a practical rule of thumb, values between 1.5 and 2.5 generally indicate no concerning autocorrelation. Autocorrelation is particularly important in time series data, where it can lead to inefficient estimates and unreliable standard errors. When detected, solutions include using autoregressive models, generalized least squares, or robust standard errors.",

    "breusch_godfrey": "The Breusch-Godfrey test examines higher-order autocorrelation in regression residuals, making it more general than the Durbin-Watson test. The null hypothesis is that there is no autocorrelation up to lag order p. A p-value less than 0.05 rejects the null hypothesis, indicating the presence of autocorrelation. The test is particularly valuable for time series data and models with lagged dependent variables, where the Durbin-Watson test may be inappropriate. The test involves an auxiliary regression of residuals on the original regressors and lagged residuals. Both Lagrange Multiplier (LM) and F-test versions are provided, with similar interpretations. When autocorrelation is detected, consider using HAC standard errors, ARIMA modeling, or generalized least squares. The 'nlags' parameter allows testing for autocorrelation at different lag orders, which is useful when the structure of autocorrelation is unknown. This test is robust to heteroscedasticity and does not require normally distributed errors.",

    "ljung_box": "The Ljung-Box test examines whether autocorrelations of residuals, up to a specified lag order, are collectively different from zero. The null hypothesis is that residuals are independently distributed (no autocorrelation). A p-value less than 0.05 for any lag rejects the null hypothesis, indicating autocorrelation at some lags. The test is a modified version of the Box-Pierce test with better small-sample properties. It is particularly useful for diagnostic checking in ARIMA modeling and for testing the adequacy of model specification in time series analysis. The test can be applied at multiple lags simultaneously, allowing for detection of seasonal patterns or complex autocorrelation structures. When autocorrelation is detected, the pattern of significant lags can guide model refinement. For example, significant autocorrelation at seasonal lags might suggest adding seasonal components to the model. The 'model_df' parameter adjusts degrees of freedom for parameters already estimated in the model, which is important for accurate inference in ARIMA models.",

    "jarque_bera": "The Jarque-Bera test assesses whether regression residuals follow a normal distribution by examining their skewness and kurtosis. Under the null hypothesis, residuals are normally distributed. A p-value less than 0.05 rejects normality, suggesting the residuals deviate significantly from a normal distribution. For large samples (n>30), non-normality may not be problematic due to the Central Limit Theorem. However, in small samples, non-normal residuals can invalidate hypothesis tests and confidence intervals. Solutions include transforming variables, using robust regression methods, or bootstrapping. A p-value greater than 0.05 suggests no significant departure from normality.",

    "shapiro_wilk": "The Shapiro-Wilk test is one of the most powerful tests for normality, especially for small to medium-sized samples (3 to 5000 observations). The null hypothesis is that the residuals are normally distributed. A p-value less than 0.05 rejects the null hypothesis, indicating non-normality. The test statistic (W) ranges from 0 to 1, with values closer to 1 indicating normality. For large samples, the test may reject normality due to small deviations that are not practically significant. In econometric applications, this test is particularly valuable for small sample inference where the Central Limit Theorem may not apply. When non-normality is detected, consider transforming variables, using robust standard errors, or employing non-parametric methods. The test should be used in conjunction with visual diagnostics like Q-Q plots to fully assess normality.",

    "dagostino_pearson": "The D'Agostino-Pearson test (also known as the omnibus K² test) combines skewness and kurtosis to produce a comprehensive test of normality. The null hypothesis is that the residuals are normally distributed. A p-value less than 0.05 rejects the null hypothesis, indicating non-normality. The test requires at least 8 observations and follows a chi-squared distribution with 2 degrees of freedom. This test is particularly useful when departures from normality might occur in either skewness or kurtosis. In econometric applications, it provides a balanced assessment of normality that is less sensitive to sample size than some alternatives. When non-normality is detected, examine the skewness and kurtosis values to determine the nature of the departure and consider appropriate transformations or robust methods. This test complements other normality tests and should be part of a comprehensive diagnostic approach.",

    "anderson_darling": "The Anderson-Darling test is a modification of the Kolmogorov-Smirnov test that gives more weight to the tails of the distribution, making it particularly sensitive to deviations in the extremes of the data. The null hypothesis is that the residuals are normally distributed. The test compares the empirical distribution function (EDF) with the cumulative distribution function (CDF) of the normal distribution. If the test statistic exceeds the critical value at a given significance level (typically 5%), reject the null hypothesis, indicating non-normality. This test is especially valuable in econometric applications where tail behavior is important, such as in financial modeling or risk assessment. When non-normality is detected, examine the pattern of deviations in Q-Q plots to determine appropriate transformations or consider models that explicitly account for heavy tails or skewness. The test provides critical values for multiple significance levels (15%, 10%, 5%, 2.5%, and 1%), allowing for flexible hypothesis testing.",

    "omnibus_test": "The Omnibus test is a chi-squared test of the skewness and kurtosis of residuals, similar to the Jarque-Bera test. The null hypothesis is that the residuals are normally distributed. A p-value less than 0.05 rejects the null hypothesis, indicating non-normality. The test statistic follows a chi-squared distribution with 2 degrees of freedom. This test is implemented in statsmodels and is routinely reported in regression summaries, making it a standard diagnostic in econometric practice. While less powerful than the Shapiro-Wilk test for small samples, it provides a quick assessment of normality that is particularly useful in routine model checking. When non-normality is detected, examine the skewness and kurtosis separately to determine the nature of the departure from normality. For large samples (n>200), minor departures from normality may be statistically significant but practically unimportant due to the Central Limit Theorem.",

    "wu_hausman": "The Wu-Hausman test examines whether variables treated as endogenous in an instrumental variables regression could actually be treated as exogenous. The null hypothesis is that all endogenous variables are exogenous. A p-value less than 0.05 rejects the null hypothesis, indicating that at least one variable is indeed endogenous and IV estimation is appropriate. The test compares OLS and IV estimates, with a significant difference suggesting endogeneity. This test is crucial for determining whether the computational complexity and potential efficiency loss of IV estimation is justified. If the test fails to reject exogeneity, OLS may be preferred for its efficiency. The test requires valid instruments that are both relevant (correlated with endogenous regressors) and exogenous (uncorrelated with the error term). In practice, this test should be conducted before choosing between OLS and IV estimation methods.",

    "wooldridge_regression": "Wooldridge's regression test is an alternative form of the Wu-Hausman test that uses a regression-based approach to test for endogeneity. The null hypothesis is that the endogenous variables are exogenous. A p-value less than 0.05 rejects the null hypothesis, indicating endogeneity. The test involves including residuals from first-stage regressions in the original model; significant coefficients on these residuals indicate endogeneity. This test is computationally simpler than the traditional Hausman test and is robust to heteroscedasticity. It can be applied to various models including limited dependent variable models. When endogeneity is detected, instrumental variables methods should be used instead of OLS. The test requires valid instruments that satisfy both relevance and exclusion restrictions. In econometric practice, this test is often preferred for its robustness and ease of implementation.",

    "sargan": "The Sargan test (also known as the J-test) examines the validity of instrumental variables by testing whether the instruments are uncorrelated with the error term. The null hypothesis is that all instruments are valid (orthogonal to the errors). A p-value less than 0.05 rejects the null hypothesis, suggesting at least one instrument is invalid. This test is only applicable when the model is overidentified (more instruments than endogenous variables). A significant result indicates that the model specification may be incorrect or some instruments may be invalid. However, the test cannot identify which specific instruments are problematic. When instrument validity is rejected, researchers should reconsider their instrument selection or model specification. The test is an essential diagnostic for IV estimation, as invalid instruments can lead to inconsistent estimates that may be worse than biased OLS estimates.",

    "durbin_wu_hausman": "The Durbin-Wu-Hausman test (often simply called the Hausman test) examines whether endogeneity is present in a regression model. The null hypothesis is that OLS estimates are consistent (variables are exogenous). A p-value less than 0.05 rejects the null hypothesis, indicating endogeneity and suggesting that instrumental variables methods are necessary. The test compares the coefficient estimates from OLS and IV methods; a significant difference indicates endogeneity. This test is fundamental in econometrics for determining the appropriate estimation method. If endogeneity is detected, IV methods should be used despite their lower efficiency compared to OLS. The test requires valid instruments and is sensitive to instrument strength. Weak instruments can lead to misleading results, so the test should be used in conjunction with tests for instrument relevance. In practice, this test helps researchers make the trade-off between consistency (IV) and efficiency (OLS)."
} 